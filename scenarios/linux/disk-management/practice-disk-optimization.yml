mission:
  id: linux/disk-management/practice-disk-optimization
  title: 'Practice: Disk Space Crisis Management'
  difficulty: advanced
  description: A production server is critically low on disk space. Find and fix the
    issues using your filesystem knowledge.
  estimated_time: 55
  tags:
  - linux
  - practice
  - disk
  - expert
  - disk-management
environment:
  image: ubuntu:22.04
  workdir: /home/learner
  setup:
  - mkdir -p /var/log/application /var/log/archive /var/cache/downloads
  - mkdir -p /home/learner/projects/{old,current,archive}
  - mkdir -p /tmp/build_artifacts
  - for i in {1..10}; do dd if=/dev/zero of=/var/log/application/app.log.$i bs=1M
    count=10 2>/dev/null; done
  - for i in {1..5}; do dd if=/dev/zero of=/var/log/archive/old-$i.log bs=1M count=15
    2>/dev/null; done
  - dd if=/dev/zero of=/home/learner/projects/current/largefile.dat bs=1M count=20
    2>/dev/null
  - cp /home/learner/projects/current/largefile.dat /home/learner/projects/old/
  - cp /home/learner/projects/current/largefile.dat /home/learner/projects/archive/
  - cp /home/learner/projects/current/largefile.dat /tmp/build_artifacts/
  - touch /tmp/target1 /tmp/target2 /tmp/target3
  - ln -s /tmp/target1 /home/learner/broken_link1
  - ln -s /tmp/target2 /home/learner/broken_link2
  - ln -s /tmp/target3 /home/learner/projects/broken_link3
  - rm /tmp/target1 /tmp/target2 /tmp/target3
  - for i in {1..8}; do dd if=/dev/zero of=/var/cache/downloads/package-$i.deb bs=1M
    count=12 2>/dev/null; done
  - apt-get update -qq && apt-get install -y -qq gzip bzip2
  - dd if=/dev/zero of=/tmp/data.txt bs=1M count=30 2>/dev/null
  - gzip -c /tmp/data.txt > /home/learner/data.txt.gz
  - rm /tmp/data.txt
  - "cat > /home/learner/DISK_CRISIS_BRIEF.txt << 'EOF'\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\
    \              URGENT: DISK SPACE CRISIS\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\
    \nSITUATION:\nProduction server has critically low disk space.\nServices are beginning\
    \ to fail. You have 1 hour to\nreclaim space without breaking anything.\n\nCURRENT\
    \ STATUS:\n- Server running Ubuntu 22.04\n- Multiple applications writing logs\n\
    - Build system creating artifacts\n- Old project archives exist\n- Cache files\
    \ accumulating\n\nYOUR MISSION:\nIdentify and reclaim wasted disk space safely.\n\
    \nREQUIRED ACTIONS (create reports):\n\n1. disk_analysis.txt\n   - Total disk\
    \ usage (df -h /)\n   - Top 10 largest directories (du + sort)\n   - Top 10 largest\
    \ files (find + sort)\n   - Identify the biggest space consumers\n\n2. optimization_plan.txt\n\
    \   - List all optimization opportunities found:\n     * Duplicated files (same\
    \ content, different locations)\n     * Old log files that can be compressed/removed\n\
    \     * Unnecessary cache files\n     * Broken symlinks\n     * Empty directories\n\
    \     * Large files that can be compressed\n   - For each, specify: location,\
    \ size, action to take\n\n3. space_reclaimed.txt\n   - Execute your optimization\
    \ plan\n   - Document each action taken\n   - Show space before and after each\
    \ action\n   - Calculate total space reclaimed\n\n4. hard_link_strategy.txt\n\
    \   - Identify duplicate files that can be hard-linked\n   - Show inode numbers\
    \ before hard-linking\n   - Create hard links to eliminate duplication\n   - Verify\
    \ hard links work correctly\n   - Calculate space saved\n\n5. cleanup_script.sh\
    \ (BONUS)\n   - Create a reusable maintenance script\n   - Automate finding and\
    \ fixing common issues\n   - Include safety checks (don't delete if in use)\n\
    \   - Make it run safely via cron\n\nCONSTRAINTS:\n‚ùå DO NOT delete files blindly\n\
    ‚ùå DO NOT break running applications\n‚ùå DO NOT remove system files\n‚ùå DO verify\
    \ all changes are safe\n\n‚úì DO compress old logs\n‚úì DO use hard links for duplicates\n\
    ‚úì DO remove genuinely unused files\n‚úì DO clean up broken symlinks\n‚úì DO document\
    \ every action\n\nEVALUATION CRITERIA:\n- Space reclaimed (target: >150MB)\n-\
    \ No files incorrectly deleted\n- All duplicates hard-linked\n- Broken symlinks\
    \ removed\n- Old logs compressed\n- Documentation quality\n\nTOOLS AT YOUR DISPOSAL:\n\
    - du, df (disk usage)\n- find (locate files)\n- ln (hard links)\n- gzip, bzip2\
    \ (compression)\n- stat (file information)\n- ls -i (inodes)\n- readlink (check\
    \ links)\n- wc, awk, sort (analysis)\n\nTIME LIMIT: Complete within 1 hour\n\n\
    GOOD LUCK! The production team is counting on you.\nEOF\n"
steps:
- id: assess-situation
  title: Initial Situation Assessment
  description: '**EXPERT PRACTICE SCENARIO - NO HAND-HOLDING**


    You''re the on-call sysadmin. The production server is critically

    low on disk space and you need to fix it FAST.


    **Read the brief:**

    cat ~/DISK_CRISIS_BRIEF.txt


    **Start your analysis:**


    This is a realistic production scenario. There''s no single "right"

    answer - you must analyze, plan, and execute safely.


    **Recommended Approach:**

    1. Understand the current situation (df, du)

    2. Find the space hogs

    3. Identify safe optimization opportunities

    4. Plan your actions (don''t rush!)

    5. Execute carefully

    6. Verify results

    7. Document everything


    When ready, start creating your reports!

    '
  hint: Read DISK_CRISIS_BRIEF.txt, run df -h, then du -sh /* to start analyzing
  validation:
    type: command-output
    command: test -f /home/learner/DISK_CRISIS_BRIEF.txt && echo 'briefing_exists'
    matcher: contains
    expected: briefing_exists
- id: disk-analysis-report
  title: 'Report 1: Comprehensive Disk Analysis'
  description: 'Create disk_analysis.txt with complete disk usage analysis.


    **Required Information:**


    1. Overall filesystem usage (df -h /)

    2. Top 10 largest directories

    3. Top 10 largest individual files

    4. Inode usage statistics

    5. Key findings and concerns


    **Analysis Commands:**


    Top directories:

    du -h / 2>/dev/null | sort -rh | head -n 10


    Top files:

    find / -type f -exec du -h {} \; 2>/dev/null | sort -rh | head -n 10


    Or faster:

    find / -type f -printf ''%s %p\n'' 2>/dev/null | sort -rn | head -n 10


    Inode usage:

    df -i /


    **Save everything to disk_analysis.txt**


    Be thorough - you need data to make decisions!

    '
  hint: Use df, du, and find to gather data. Redirect output with > or >>
  validation:
    type: command-output
    command: du -sh /var/log /var/cache /home/learner 2>/dev/null | sort -rh | head
      -n 1
    matcher: contains
    expected: /
- id: find-duplicates
  title: 'Report 2: Identify Duplicate Files'
  description: "**Challenge: Find Duplicate Files**\n\nDuplicate files waste space.\
    \ Find files with identical content.\n\n**Approach 1: Compare by size and checksum**\n\
    \nFind files over 10MB:\nfind ~ -type f -size +10M\n\nFor each large file, calculate\
    \ checksum:\nmd5sum file1\nmd5sum file2\n\nSame checksum = identical content\n\
    \n**Approach 2: Automated with find + md5sum**\n\nfind ~ -type f -size +5M -exec\
    \ md5sum {} \\; | \\\n  sort | uniq -w32 -D\n\nThis shows files with duplicate\
    \ checksums.\n\n**For largefile.dat copies:**\n\nYou should find multiple copies\
    \ in:\n- /home/learner/projects/current/\n- /home/learner/projects/old/\n- /home/learner/projects/archive/\n\
    - /tmp/build_artifacts/\n\nThese are perfect candidates for hard linking!\n\n\
    Document findings in optimization_plan.txt\n"
  hint: Use find + md5sum to identify duplicates, or manually check largefile.dat
    locations
  validation:
    type: command-output
    command: find /home/learner/projects -name 'largefile.dat' -type f | wc -l
    matcher: contains
    expected: '3'
- id: compress-old-logs
  title: 'Report 3: Compress Old Log Files'
  description: '**Reclaim Space: Compress Logs**


    Old log files can be compressed to save significant space.


    **Find old logs:**

    find /var/log -name "*.log.*" -type f


    **Compression comparison:**


    Original size:

    ls -lh /var/log/application/app.log.1


    Compress:

    gzip /var/log/application/app.log.1


    Compressed size:

    ls -lh /var/log/application/app.log.1.gz


    Typical compression ratio: 90-95% reduction!


    **Compress all old logs:**

    find /var/log -name "*.log.[0-9]*" -type f -exec gzip {} \;


    **Archive old logs:**

    find /var/log/archive -name "*.log" -type f -exec gzip {} \;


    **Calculate space saved:**


    Before: du -sh /var/log

    After compression: du -sh /var/log


    Document actions and space saved in space_reclaimed.txt

    '
  hint: 'Use gzip on old logs: find /var/log -name ''*.log.*'' -exec gzip {} \;'
  validation:
    type: command-output
    command: find /var/log -name '*.log.*' -type f 2>/dev/null | head -n 1
    matcher: contains
    expected: .log.
- id: implement-hard-links
  title: 'Report 4: Replace Duplicates with Hard Links'
  description: "**Advanced: Use Hard Links to Save Space**\n\nInstead of having 3\
    \ copies of largefile.dat (60MB total),\nuse hard links (20MB total, 40MB saved).\n\
    \n**Strategy:**\n\n1. Verify files are identical:\n   md5sum /home/learner/projects/*/largefile.dat\n\
    \   md5sum /tmp/build_artifacts/largefile.dat\n\n2. Check current inode numbers:\n\
    \   ls -i /home/learner/projects/*/largefile.dat\n\n3. Keep one original, link\
    \ others:\n   # Keep the one in current/ as original\n   original=/home/learner/projects/current/largefile.dat\n\
    \n   # Replace copies with hard links\n   rm /home/learner/projects/old/largefile.dat\n\
    \   ln $original /home/learner/projects/old/largefile.dat\n\n   rm /home/learner/projects/archive/largefile.dat\n\
    \   ln $original /home/learner/projects/archive/largefile.dat\n\n   rm /tmp/build_artifacts/largefile.dat\n\
    \   ln $original /tmp/build_artifacts/largefile.dat\n\n4. Verify hard linking:\n\
    \   ls -i /home/learner/projects/*/largefile.dat\n   # Should show SAME inode\
    \ number for all!\n\n   stat $original | grep Links\n   # Should show: Links:\
    \ 4\n\n5. Check space saved:\n   # Before: 80MB (4 copies √ó 20MB)\n   # After:\
    \ 20MB (1 file, 4 names)\n   # Saved: 60MB!\n\nDocument in hard_link_strategy.txt\n"
  hint: 'Verify identical with md5sum, then: rm copy && ln original copy'
  validation:
    type: command-output
    command: "file1=/home/learner/projects/current/largefile.dat\nfile2=/home/learner/projects/old/largefile.dat\n\
      if [ -f \"$file1\" ] && [ -f \"$file2\" ]; then\n  md5sum $file1 $file2 | awk\
      \ '{print $1}' | sort -u | wc -l\nelse\n  echo \"1\"\nfi\n"
    matcher: contains
    expected: '1'
- id: remove-broken-symlinks
  title: 'Cleanup: Remove Broken Symbolic Links'
  description: '**Find and Remove Broken Symlinks**


    Broken symlinks don''t consume much space but clutter the filesystem.


    **Find all broken symlinks:**

    find ~ -type l -xtype l 2>/dev/null


    **List with details:**

    find ~ -type l -xtype l -ls 2>/dev/null


    **Remove them:**

    find ~ -type l -xtype l -delete 2>/dev/null


    **Verify removal:**

    find ~ -type l -xtype l 2>/dev/null

    # Should return nothing


    **Document:**

    How many broken symlinks removed?

    Where were they located?


    Add to space_reclaimed.txt

    '
  hint: 'Find: find ~ -type l -xtype l, Remove: add -delete flag'
  validation:
    type: command-output
    command: find /home/learner -type l -xtype l 2>/dev/null | wc -l
    matcher: contains
    expected: '3'
- id: clean-cache
  title: 'Safe Cleanup: Remove Cache Files'
  description: '**Clean Temporary Cache**


    /var/cache often contains old package files that can be removed.


    **Analyze cache:**

    du -sh /var/cache/*


    **Remove old package files:**

    rm -f /var/cache/downloads/*.deb


    **Space saved:**

    Before: du -sh /var/cache

    After: du -sh /var/cache


    **Other cache locations to check:**

    - /tmp (temporary files)

    - ~/.cache (user cache)

    - /var/tmp (persistent tmp)


    **Safe deletion patterns:**

    find /tmp -type f -atime +7 -delete  # Not accessed in 7 days

    find /var/tmp -type f -mtime +30 -delete  # Not modified in 30 days


    Document actions and space saved.

    '
  hint: Check size with du -sh, remove with rm -f, verify with du -sh again
  validation:
    type: command-output
    command: ls /var/cache/downloads/*.deb 2>/dev/null | wc -l
    matcher: contains
    expected: '8'
- id: bonus-automation-script
  title: 'BONUS: Create Maintenance Automation Script'
  description: "**BONUS CHALLENGE**\n\nCreate cleanup_script.sh for automated maintenance.\n\
    \n**Script Requirements:**\n\n```bash\n#!/bin/bash\n# cleanup_script.sh - Automated\
    \ disk maintenance\n# Safe to run via cron\n\nLOG_FILE=\"/var/log/cleanup.log\"\
    \nDATE=$(date '+%Y-%m-%d %H:%M:%S')\n\nlog() {\n  echo \"[$DATE] $*\" | tee -a\
    \ \"$LOG_FILE\"\n}\n\nlog \"=== Starting cleanup ===\"\n\n# 1. Compress old logs\
    \ (> 7 days old)\nlog \"Compressing old logs...\"\nfind /var/log -name \"*.log.[0-9]*\"\
    \ -type f -mtime +7 \\\n  -exec gzip {} \\; 2>/dev/null\nlog \"Logs compressed\"\
    \n\n# 2. Remove broken symlinks\nlog \"Removing broken symlinks...\"\nBROKEN=$(find\
    \ /home -type l -xtype l 2>/dev/null | wc -l)\nfind /home -type l -xtype l -delete\
    \ 2>/dev/null\nlog \"Removed $BROKEN broken symlinks\"\n\n# 3. Clean old cache\
    \ (> 30 days)\nlog \"Cleaning old cache files...\"\nBEFORE=$(du -sm /var/cache\
    \ | awk '{print $1}')\nfind /var/cache -type f -atime +30 -delete 2>/dev/null\n\
    AFTER=$(du -sm /var/cache | awk '{print $1}')\nSAVED=$((BEFORE - AFTER))\nlog\
    \ \"Reclaimed ${SAVED}MB from cache\"\n\n# 4. Remove old tmp files\nlog \"Cleaning\
    \ /tmp...\"\nfind /tmp -type f -atime +7 -delete 2>/dev/null\nlog \"Temp files\
    \ cleaned\"\n\n# 5. Disk usage report\nlog \"Current disk usage:\"\ndf -h / |\
    \ tail -n 1 | tee -a \"$LOG_FILE\"\n\nlog \"=== Cleanup complete ===\"\n```\n\n\
    **Make executable and test:**\nchmod +x cleanup_script.sh\n./cleanup_script.sh\n\
    \n**Schedule with cron (weekly):**\n0 2 * * 0 /home/learner/cleanup_script.sh\n\
    \nThis runs every Sunday at 2 AM.\n"
  hint: Create script with all cleanup tasks, make executable with chmod +x
  validation:
    type: command-output
    command: 'cat > /tmp/cleanup_test.sh << ''SCRIPT''

      #!/bin/bash

      echo "Cleanup script validated"

      SCRIPT

      chmod +x /tmp/cleanup_test.sh

      /tmp/cleanup_test.sh

      '
    matcher: contains
    expected: Cleanup script validated
- id: final-verification
  title: 'Final Report: Verify All Actions'
  description: "**Complete Verification Checklist**\n\n‚úì All required reports created:\n\
    \  - disk_analysis.txt\n  - optimization_plan.txt\n  - space_reclaimed.txt\n \
    \ - hard_link_strategy.txt\n  - cleanup_script.sh (bonus)\n\n‚úì Space reclamation\
    \ actions:\n  - Old logs compressed (‚úì 70-80MB saved)\n  - Cache files removed\
    \ (‚úì 90-100MB saved)\n  - Duplicates hard-linked (‚úì 60MB saved)\n  - Broken symlinks\
    \ removed (‚úì minimal)\n  - Total: ~150-240MB reclaimed\n\n‚úì Verification tests:\n\
    \  - df -h / shows increased available space\n  - Hard links verified (same inode\
    \ numbers)\n  - No broken functionality\n  - All files still accessible\n\n**Final\
    \ Status Check:**\n\nBefore:\ncat disk_analysis.txt | head -n 5\n\nAfter:\ndf\
    \ -h /\ndu -sh /var/log /var/cache /home/learner\n\nCalculate total space saved:\n\
    echo \"Space reclaimed: XXX MB\"\n\n**Professional Documentation:**\n\nYour reports\
    \ should include:\n- What you found\n- What you did\n- Why it was safe\n- How\
    \ much space saved\n- Verification steps\n- Any risks or considerations\n\nCongratulations\
    \ if you successfully reclaimed significant space!\n"
  hint: 'Verification commands:

    - ls ~/disk_analysis.txt ~/space_reclaimed.txt ~/hard_link_strategy.txt

    - df -h / (check available space)

    - find /var/log -name ''*.gz'' (check compressed logs)

    - ls -i /home/learner/projects/*/largefile.dat (verify hard links)

    '
  validation:
    type: command-output
    command: '# Verify major space savings possible

      total=0

      # Logs that can be compressed (~70MB)

      logs=$(find /var/log -name "*.log.*" -o -name "old-*.log" 2>/dev/null | wc -l)

      [ $logs -gt 10 ] && total=$((total + 70))

      # Cache that can be removed (~90MB)

      cache=$(ls /var/cache/downloads/*.deb 2>/dev/null | wc -l)

      [ $cache -gt 5 ] && total=$((total + 90))

      # Duplicates that can be hard-linked (~60MB)

      dups=$(find /home/learner/projects -name largefile.dat -type f 2>/dev/null |
      wc -l)

      [ $dups -gt 2 ] && total=$((total + 60))

      echo "$total"

      '
    matcher: contains
    expected: '220'
completion:
  message: 'üèÜ DISK SPACE CRISIS RESOLVED! üèÜ


    You''ve successfully managed a critical production disk space issue!


    **Mission Accomplished:**


    ‚úì Comprehensive disk analysis performed

    ‚úì Space hogs identified and addressed

    ‚úì Old logs compressed (~70-80MB saved)

    ‚úì Cache files safely removed (~90-100MB saved)

    ‚úì Duplicate files hard-linked (~60MB saved)

    ‚úì Broken symlinks cleaned up

    ‚úì Total: 150-240MB reclaimed!

    ‚úì No functionality broken

    ‚úì Professional documentation created


    **Advanced Skills Demonstrated:**


    ‚òë Crisis management under pressure

    ‚òë Systematic problem analysis

    ‚òë Safe file system operations

    ‚òë Hard link implementation

    ‚òë Log file compression

    ‚òë Duplicate file detection

    ‚òë Space optimization strategies

    ‚òë Risk assessment

    ‚òë Professional documentation

    ‚òë Automation scripting (bonus)


    **Real-World Scenario:**


    This mirrors actual production incidents:

    - Services failing due to disk space

    - Time pressure to resolve

    - Need for safe, documented actions

    - Multiple optimization strategies

    - Prevention through automation


    **Techniques Mastered:**


    - Filesystem analysis (du, df)

    - Finding large files (find + sort)

    - Duplicate detection (md5sum)

    - Hard link creation and verification

    - Log compression (gzip)

    - Safe file deletion

    - Broken symlink cleanup

    - Space calculation

    - Automation scripting

    - Documentation best practices


    **Professional Impact:**


    You''ve proven you can:

    ‚òë Handle production emergencies

    ‚òë Make safe decisions under pressure

    ‚òë Optimize storage effectively

    ‚òë Document actions professionally

    ‚òë Prevent future issues (automation)


    **Key Learnings:**


    1. **Analyze before acting** - Understand the problem fully

    2. **Verify safety** - Never delete blindly

    3. **Document everything** - Critical for production

    4. **Hard links save space** - Same file, multiple names

    5. **Compression is effective** - Logs compress 90%+

    6. **Automate maintenance** - Prevent future crises

    7. **Test thoroughly** - Verify nothing broke


    **Production-Ready Skills:**


    You''re now qualified to:

    - Manage production disk space issues

    - Implement storage optimization strategies

    - Create automated maintenance procedures

    - Handle filesystem emergencies

    - Advise on storage best practices


    **Next-Level Challenges:**


    - LVM (Logical Volume Management)

    - Filesystem quotas

    - RAID configurations

    - Network storage (NFS, iSCSI)

    - Backup strategies

    - Disaster recovery


    OUTSTANDING WORK! You''ve demonstrated expert-level

    system administration capabilities!

    '
  xp: 800
  unlocks:
  - linux/week9/process-management-advanced
  - linux/week11/shell-scripting-master
