mission:
  id: "linux/week13/practice-log-parsing"
  title: "Production Log Analysis and Transformation"
  difficulty: expert
  description: "Parse, analyze, and transform production logs using regex, grep, sed, and shell scripting"
  estimated_time: 55
  tags:
    - linux
    - practice
    - regex
    - grep
    - sed
    - awk
    - expert
    - week13
    - cst8207

environment:
  image: "ubuntu:22.04"
  workdir: "/opt/log-analyzer"
  setup:
    - "apt-get update -qq && apt-get install -y -qq grep sed gawk"
    - "mkdir -p /opt/log-analyzer /var/log/application /var/log/processed"
    - |
      # Generate realistic application log
      cat > /var/log/application/app.log << 'EOF'
      2026-01-09 10:15:23.456 [INFO] com.app.service.UserService - User john.doe@example.com logged in from 192.168.1.100
      2026-01-09 10:16:45.789 [ERROR] com.app.database.ConnectionPool - Connection timeout to db-prod-01.internal:5432 after 30000ms
      2026-01-09 10:17:12.234 [INFO] com.app.api.AuthController - Authentication successful for user_id=12345
      2026-01-09 10:18:55.678 [WARN] com.app.cache.RedisClient - Cache miss rate: 45.5% (threshold: 30%)
      2026-01-09 10:19:03.123 [ERROR] com.app.service.PaymentService - Payment processing failed: java.lang.NullPointerException at line 234
      2026-01-09 10:20:31.456 [INFO] com.app.api.UserController - GET /api/users?page=1&limit=50 responded 200 in 145ms
      2026-01-09 10:21:47.789 [DEBUG] com.app.util.CacheHelper - Cache key user:12345:profile retrieved in 2ms
      2026-01-09 10:22:18.234 [ERROR] com.app.external.EmailService - SMTP connection failed to smtp.provider.com:587
      2026-01-09 10:23:56.567 [INFO] com.app.scheduler.BackupJob - Backup completed successfully, size: 2.3GB, duration: 450s
      2026-01-09 10:24:42.890 [WARN] com.app.monitor.HealthCheck - Disk usage on /data partition: 87% (critical threshold: 85%)
      2026-01-09 10:25:19.123 [INFO] com.app.service.UserService - User jane.smith@company.org logged out after 3600s session
      2026-01-09 10:26:34.456 [ERROR] com.app.api.OrderController - Order creation failed for customer_id=67890, error: insufficient_inventory
      2026-01-09 10:27:51.789 [INFO] com.app.security.AuditLogger - Failed login attempt from 203.0.113.45 for user: admin
      2026-01-09 10:28:12.234 [CRITICAL] com.app.database.HealthMonitor - Database connection pool exhausted (0/100 available)
      2026-01-09 10:29:45.567 [INFO] com.app.api.ProductController - POST /api/products responded 201 in 234ms
      2026-01-09 10:30:23.890 [WARN] com.app.service.RateLimiter - Rate limit exceeded for IP 192.168.1.150, requests: 1050/1000
      2026-01-09 10:31:56.123 [ERROR] com.app.external.PaymentGateway - Transaction declined: card_expired, transaction_id=TXN-98765
      2026-01-09 10:32:19.456 [INFO] com.app.messaging.KafkaConsumer - Processed 5000 messages from topic orders.created in 12000ms
      2026-01-09 10:33:42.789 [DEBUG] com.app.util.TokenValidator - JWT token validated for user_id=12345, expires_in=3600s
      2026-01-09 10:34:18.234 [ERROR] com.app.service.NotificationService - Push notification failed: device_token_invalid for user_id=11111
      EOF
    - |
      # Generate access log
      cat > /var/log/application/access.log << 'EOF'
      192.168.1.100 - john.doe [09/Jan/2026:10:15:23 +0000] "GET /dashboard HTTP/1.1" 200 15234 "https://app.example.com/login" "Mozilla/5.0"
      192.168.1.101 - jane.smith [09/Jan/2026:10:16:45 +0000] "POST /api/login HTTP/1.1" 200 456 "-" "axios/0.21.1"
      10.0.0.55 - - [09/Jan/2026:10:17:12 +0000] "GET /api/users/12345 HTTP/1.1" 200 1234 "-" "curl/7.68.0"
      192.168.1.102 - admin [09/Jan/2026:10:18:55 +0000] "GET /admin/panel HTTP/1.1" 403 234 "-" "Mozilla/5.0"
      172.16.0.10 - service [09/Jan/2026:10:19:03 +0000] "POST /api/payments HTTP/1.1" 500 123 "-" "Java/11.0.2"
      192.168.1.100 - john.doe [09/Jan/2026:10:20:31 +0000] "GET /api/users?page=1&limit=50 HTTP/1.1" 200 8765 "-" "Mozilla/5.0"
      203.0.113.45 - - [09/Jan/2026:10:27:51 +0000] "POST /api/login HTTP/1.1" 401 89 "-" "python-requests/2.25.1"
      192.168.1.150 - user [09/Jan/2026:10:30:23 +0000] "GET /api/products HTTP/1.1" 429 156 "-" "Mozilla/5.0"
      10.0.0.56 - system [09/Jan/2026:10:32:19 +0000] "POST /api/webhooks/kafka HTTP/1.1" 200 45 "-" "KafkaProducer/2.8.0"
      192.168.1.103 - bob [09/Jan/2026:10:35:42 +0000] "DELETE /api/users/99999 HTTP/1.1" 404 234 "-" "curl/7.68.0"
      EOF

steps:
  - id: "understand-requirements"
    title: "Analyze Requirements"
    description: |
      **SCENARIO: Production Log Analysis**

      You're a DevOps engineer investigating production issues and preparing
      logs for security audit. You must parse, analyze, and transform logs using
      regex, grep, and sed.

      **Available logs:**
      /var/log/application/app.log     - Application logs (structured)
      /var/log/application/access.log  - HTTP access logs (Apache format)

      **Your Tasks:**

      1. **Error Analysis**
         - Extract all ERROR and CRITICAL entries
         - Count errors by category
         - Identify top error sources
         - Calculate error rate

      2. **Security Audit**
         - Anonymize PII (emails, IPs)
         - Identify failed authentication attempts
         - Find rate-limiting violations
         - Detect suspicious patterns

      3. **Performance Metrics**
         - Extract API response times
         - Identify slow endpoints (>200ms)
         - Calculate average response time
         - Find status code distribution

      4. **Report Generation**
         - Create executive summary
         - Generate anonymized logs for audit
         - Produce metrics dashboard
         - Alert on critical issues

      **Tools available:**
      - grep (with extended regex)
      - sed (stream editor)
      - awk (text processing)
      - Basic shell commands

      **Deliverables:**
      - error_summary.txt
      - anonymized_logs.txt
      - security_report.txt
      - performance_metrics.txt
      - critical_alerts.txt

      **Explore the logs first:**
      wc -l /var/log/application/*.log
      head -5 /var/log/application/app.log
      head -5 /var/log/application/access.log

      **No step-by-step guidance!**
      This is real-world log analysis. Plan and execute your solution.
    hint: "Analyze log structure before building patterns"
    validation:
      type: "command-output"
      command: "wc -l /var/log/application/*.log | tail -1"
      matcher: "contains"
      expected: "total"

  - id: "extract-errors"
    title: "Extract and Analyze Errors"
    description: |
      **Task: Create comprehensive error analysis**

      Create: /opt/log-analyzer/error_summary.txt

      **Requirements:**

      âœ“ Extract all ERROR and CRITICAL level entries
      âœ“ Count total errors vs total log entries (error rate)
      âœ“ Group errors by service/component
      âœ“ List unique error messages
      âœ“ Identify top 3 error sources
      âœ“ Extract stack traces or error details
      âœ“ Timestamp range of errors
      âœ“ Format as readable report

      **Report format:**
      ```
      ==============================================
      ERROR ANALYSIS REPORT
      Generated: YYYY-MM-DD HH:MM:SS
      ==============================================

      SUMMARY:
      - Total log entries: X
      - Error entries: X (X.X%)
      - Critical entries: X
      - Time range: HH:MM:SS to HH:MM:SS

      ERROR DISTRIBUTION:
      com.app.database: X errors
      com.app.external: X errors
      com.app.service: X errors

      TOP ERRORS:
      1. [ERROR] com.app.database.ConnectionPool - Connection timeout (2 occurrences)
      2. [ERROR] com.app.service.PaymentService - Payment processing failed (1 occurrence)
      ...

      CRITICAL ISSUES:
      [CRITICAL] com.app.database.HealthMonitor - Database connection pool exhausted

      RECOMMENDATIONS:
      - Investigate database connectivity
      - Review payment service error handling
      - Monitor connection pool usage
      ==============================================
      ```

      **Hints:**
      - Use grep -E for pattern matching
      - Use wc -l for counting
      - Use sed to extract components
      - Use sort | uniq -c for frequency
      - Combine commands with pipes

      **Success criteria:**
      - File exists with proper format
      - Accurate error counts
      - Proper grouping by component
      - Actionable insights included

      **Test your analysis:**
      cat /opt/log-analyzer/error_summary.txt
    hint: "Use grep for extraction, sed for parsing, pipes for analysis"
    validation:
      type: "file-exists"
      path: "/opt/log-analyzer/error_summary.txt"

  - id: "anonymize-logs"
    title: "Anonymize Sensitive Data"
    description: |
      **Task: Create anonymized log file**

      Create: /var/log/processed/anonymized_app.log

      **Requirements:**

      âœ“ Redact all email addresses: user@domain.com â†’ user@REDACTED
      âœ“ Anonymize IP addresses: 192.168.1.100 â†’ XXX.XXX.XXX.XXX
      âœ“ Mask user IDs: user_id=12345 â†’ user_id=XXXXX
      âœ“ Redact sensitive paths: db-prod-01.internal â†’ db-REDACTED
      âœ“ Keep log structure intact
      âœ“ Preserve timestamps
      âœ“ Maintain log level markers
      âœ“ Keep error messages readable

      **What to anonymize:**
      - Email addresses (PII)
      - IP addresses (can identify users)
      - User IDs (can link to real users)
      - Internal hostnames (security risk)
      - Customer IDs (PII)
      - Transaction IDs (potentially sensitive)

      **What to keep:**
      - Timestamps
      - Log levels
      - Component names
      - Error types
      - Metrics (percentages, durations)
      - HTTP status codes
      - HTTP methods

      **Example transformation:**

      Before:
      2026-01-09 10:15:23.456 [INFO] com.app.service.UserService - User john.doe@example.com logged in from 192.168.1.100

      After:
      2026-01-09 10:15:23.456 [INFO] com.app.service.UserService - User REDACTED@REDACTED logged in from XXX.XXX.XXX.XXX

      **Use sed for transformations:**

      sed 's/pattern/replacement/g' input.log > output.log

      **Common patterns:**
      Email: [a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]+
      IP: ([0-9]{1,3}\.){3}[0-9]{1,3}
      User ID: user_id=[0-9]+
      Customer ID: customer_id=[0-9]+

      **Success criteria:**
      - All PII removed
      - Log structure preserved
      - Still analyzable
      - Consistent redaction

      **Create anonymized log:**
      Use sed with multiple substitutions
      Verify with: grep -E '@|[0-9]{1,3}\.[0-9]{1,3}' anonymized_app.log
      Should show REDACTED or XXX only
    hint: "Chain multiple sed substitutions to redact different patterns"
    validation:
      type: "file-exists"
      path: "/var/log/processed/anonymized_app.log"

  - id: "security-audit"
    title: "Generate Security Report"
    description: |
      **Task: Create security analysis report**

      Create: /opt/log-analyzer/security_report.txt

      **Requirements:**

      âœ“ Identify failed authentication attempts
      âœ“ List suspicious IP addresses (multiple failures)
      âœ“ Find rate-limiting violations
      âœ“ Detect 403 Forbidden access attempts
      âœ“ Identify unusual HTTP methods
      âœ“ Find admin panel access attempts
      âœ“ Calculate failed login rate
      âœ“ Recommend security actions

      **Report format:**
      ```
      ==============================================
      SECURITY AUDIT REPORT
      Generated: YYYY-MM-DD HH:MM:SS
      ==============================================

      AUTHENTICATION FAILURES:
      - Failed login attempts: X
      - Unique source IPs: X
      - Targeted accounts: admin, ...

      SUSPICIOUS IPs:
      203.0.113.45 - 1 failed authentication attempts
      ...

      RATE LIMITING:
      - Rate limit violations: X
      - Affected IPs: ...
      - Max requests: X (limit: 1000)

      UNAUTHORIZED ACCESS:
      - 403 Forbidden responses: X
      - Targeted endpoints: /admin/panel, ...

      SECURITY EVENTS:
      [10:27:51] Failed login attempt from 203.0.113.45 for user: admin
      [10:30:23] Rate limit exceeded for IP 192.168.1.150

      RECOMMENDATIONS:
      - Block IP 203.0.113.45 (failed admin login)
      - Review rate limits for user accounts
      - Enable MFA for admin accounts
      - Investigate 403 patterns
      ==============================================
      ```

      **Analysis patterns:**

      Failed logins:
      grep -i 'failed login' app.log
      grep ' 401 ' access.log

      Rate limiting:
      grep -i 'rate limit' app.log
      grep ' 429 ' access.log

      Forbidden access:
      grep ' 403 ' access.log

      **Extract information:**
      - Use grep to find events
      - Use sed to extract IPs and users
      - Use sort | uniq -c for frequency
      - Use awk for calculations

      **Success criteria:**
      - Identifies all security events
      - Calculates accurate statistics
      - Provides actionable recommendations
      - Prioritizes by severity
    hint: "Grep for security events, extract IPs/users, count occurrences"
    validation:
      type: "file-exists"
      path: "/opt/log-analyzer/security_report.txt"

  - id: "performance-metrics"
    title: "Extract Performance Metrics"
    description: |
      **Task: Generate performance analysis**

      Create: /opt/log-analyzer/performance_metrics.txt

      **Requirements:**

      âœ“ Extract API response times from logs
      âœ“ Calculate average response time
      âœ“ Identify slow endpoints (>200ms)
      âœ“ Count requests by HTTP status code
      âœ“ Calculate success rate (2xx / total)
      âœ“ Find 5xx server errors
      âœ“ Identify slowest operation
      âœ“ Generate performance summary

      **Report format:**
      ```
      ==============================================
      PERFORMANCE METRICS REPORT
      Generated: YYYY-MM-DD HH:MM:SS
      ==============================================

      API RESPONSE TIMES:
      - Total API calls: X
      - Average response time: XXXms
      - Fastest response: XXms
      - Slowest response: XXms

      SLOW ENDPOINTS (>200ms):
      - POST /api/products: 234ms
      - GET /api/users: 145ms (borderline)

      HTTP STATUS DISTRIBUTION:
      - 2xx Success: X (XX.X%)
      - 4xx Client Errors: X (XX.X%)
      - 5xx Server Errors: X (XX.X%)

      ERROR RATE:
      - Total requests: X
      - Failed requests: X
      - Success rate: XX.X%

      TOP ENDPOINTS:
      GET /api/users - X requests
      POST /api/products - X requests
      ...

      PERFORMANCE CONCERNS:
      - 1 endpoint exceeding 200ms threshold
      - X 5xx errors requiring investigation
      - Success rate below 95% target

      RECOMMENDATIONS:
      - Optimize POST /api/products endpoint
      - Investigate 5xx errors in payments service
      - Add caching for frequently accessed endpoints
      ==============================================
      ```

      **Extraction patterns:**

      Response times:
      grep -oE 'responded [0-9]+ in [0-9]+ms' app.log

      HTTP status codes:
      grep -oE ' [0-9]{3} ' access.log

      **Calculations with awk:**

      Average response time:
      grep -oE 'in [0-9]+ms' app.log | awk -F'[^0-9]+' '{sum+=$2; count++} END {print sum/count}'

      Status code distribution:
      grep -oE ' [0-9]{3} ' access.log | sort | uniq -c | sort -rn

      **Success criteria:**
      - Accurate time calculations
      - Proper status code grouping
      - Identifies performance issues
      - Provides optimization recommendations
    hint: "Extract times with grep, calculate averages with awk"
    validation:
      type: "file-exists"
      path: "/opt/log-analyzer/performance_metrics.txt"

  - id: "critical-alerts"
    title: "Generate Critical Alerts"
    description: |
      **Task: Create critical alert dashboard**

      Create: /opt/log-analyzer/critical_alerts.txt

      **Requirements:**

      âœ“ Flag all CRITICAL level entries
      âœ“ Identify resource exhaustion (disk, memory, connections)
      âœ“ Find cascading failures
      âœ“ Detect service outages
      âœ“ Calculate downtime duration
      âœ“ Assess business impact
      âœ“ Prioritize by severity
      âœ“ Provide immediate action items

      **Report format:**
      ```
      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
      â•‘         CRITICAL ALERTS DASHBOARD                 â•‘
      â•‘            IMMEDIATE ACTION REQUIRED             â•‘
      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      SEVERITY: HIGH â– â– â– â– â– 

      [1] DATABASE CONNECTION POOL EXHAUSTED
      Time: 10:28:12
      Impact: All database operations failing
      Affected: All services requiring database
      Action: Restart connection pool / Scale database
      Status: ğŸ”´ ACTIVE

      [2] DISK USAGE CRITICAL (87%)
      Time: 10:24:42
      Impact: System may become unresponsive
      Affected: /data partition
      Action: Clean up logs / Expand storage
      Status: ğŸŸ¡ WARNING

      [3] MULTIPLE PAYMENT FAILURES
      Time: 10:19:03, 10:31:56
      Impact: Revenue loss, customer dissatisfaction
      Affected: PaymentService, PaymentGateway
      Action: Investigate payment provider
      Status: ğŸ”´ ACTIVE

      CASCADING FAILURES DETECTED:
      Database timeout â†’ Payment failures â†’ Connection pool exhaustion

      ESTIMATED IMPACT:
      - Downtime: ~10 minutes (10:19 - 10:29)
      - Failed transactions: 2+
      - Affected users: Unknown (requires correlation)

      IMMEDIATE ACTIONS:
      1. Restart database connection pool (URGENT)
      2. Free disk space on /data partition
      3. Contact payment provider support
      4. Enable circuit breaker for payment service
      5. Scale database connections

      ONCALL ESCALATION:
      - Database team: PAGED
      - Payment team: NOTIFIED
      - Management: ALERT SENT

      FOLLOW-UP:
      - Post-incident review required
      - Update runbooks
      - Improve monitoring thresholds
      ```

      **Pattern detection:**

      Critical logs:
      grep '\[CRITICAL\]' app.log

      Resource issues:
      grep -iE 'exhausted|critical|threshold|exceeded' app.log

      Failure patterns:
      grep -E 'ERROR.*failed|ERROR.*timeout' app.log

      **Correlation analysis:**
      - Group related errors by time
      - Identify cause-effect relationships
      - Calculate business impact

      **Success criteria:**
      - All critical issues identified
      - Clear prioritization
      - Actionable steps provided
      - Business impact assessed
      - Formatted for executive review
    hint: "Grep for CRITICAL and resource keywords, correlate by time"
    validation:
      type: "file-exists"
      path: "/opt/log-analyzer/critical_alerts.txt"

  - id: "automation-script"
    title: "Create Analysis Automation Script"
    description: |
      **Task: Build automated log analysis tool**

      Create: /opt/log-analyzer/analyze_logs.sh

      **Requirements:**

      âœ“ Accept log file path as argument
      âœ“ Run all analyses automatically
      âœ“ Generate all required reports
      âœ“ Handle errors gracefully
      âœ“ Provide progress indicators
      âœ“ Support multiple log formats
      âœ“ Generate summary dashboard
      âœ“ Exit with appropriate codes

      **Script structure:**
      ```bash
      #!/bin/bash

      # Log Analysis Automation Script
      # Usage: ./analyze_logs.sh [log_file] [output_dir]

      set -euo pipefail

      # Configuration
      LOG_FILE="${1:-/var/log/application/app.log}"
      OUTPUT_DIR="${2:-/opt/log-analyzer}"
      TIMESTAMP=$(date +%Y%m%d_%H%M%S)

      # Functions
      analyze_errors() {
        # Extract and analyze errors
        ...
      }

      anonymize_logs() {
        # Anonymize sensitive data
        ...
      }

      security_audit() {
        # Generate security report
        ...
      }

      performance_metrics() {
        # Calculate performance metrics
        ...
      }

      critical_alerts() {
        # Identify critical issues
        ...
      }

      generate_dashboard() {
        # Create summary dashboard
        ...
      }

      # Main execution
      main() {
        echo "Starting log analysis..."

        echo "[1/6] Analyzing errors..."
        analyze_errors

        echo "[2/6] Anonymizing logs..."
        anonymize_logs

        echo "[3/6] Security audit..."
        security_audit

        echo "[4/6] Performance metrics..."
        performance_metrics

        echo "[5/6] Critical alerts..."
        critical_alerts

        echo "[6/6] Generating dashboard..."
        generate_dashboard

        echo "Analysis complete. Reports in: $OUTPUT_DIR"
      }

      main "$@"
      ```

      **Dashboard format:**
      ```
      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
      â•‘       LOG ANALYSIS DASHBOARD                 â•‘
      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      Analysis Date: 2026-01-09 10:35:00
      Log File: /var/log/application/app.log
      Total Entries: 20

      STATUS: ğŸ”´ CRITICAL ISSUES DETECTED

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ ERROR SUMMARY                                â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚ Error Rate: 30.0% (6/20 entries)            â”‚
      â”‚ Critical: 1                                  â”‚
      â”‚ Top Source: com.app.database (2 errors)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ SECURITY STATUS                              â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚ Failed Logins: 1                            â”‚
      â”‚ Rate Limit Violations: 1                    â”‚
      â”‚ Suspicious IPs: 2                           â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ PERFORMANCE METRICS                          â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚ Avg Response Time: 189ms                    â”‚
      â”‚ Slow Endpoints: 1                           â”‚
      â”‚ Success Rate: 75.0%                         â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      GENERATED REPORTS:
      âœ“ error_summary.txt
      âœ“ anonymized_app.log
      âœ“ security_report.txt
      âœ“ performance_metrics.txt
      âœ“ critical_alerts.txt

      NEXT STEPS:
      1. Review critical alerts immediately
      2. Address database connection issues
      3. Investigate security incidents
      4. Optimize slow endpoints
      ```

      **Make script executable:**
      chmod +x /opt/log-analyzer/analyze_logs.sh

      **Test script:**
      ./analyze_logs.sh /var/log/application/app.log

      **Success criteria:**
      - Script runs without errors
      - All reports generated
      - Dashboard summarizes findings
      - Proper error handling
      - Reusable and maintainable
    hint: "Combine all analysis functions into one automated script"
    validation:
      type: "command-output"
      command: "test -x /opt/log-analyzer/analyze_logs.sh && echo 'executable'"
      matcher: "contains"
      expected: "executable"

completion:
  message: |
    ğŸ‰ LOG ANALYSIS EXPERTISE ACHIEVED! ğŸ‰

    Outstanding work! You've mastered production log analysis!

    **What You've Accomplished:**

    âœ“ Comprehensive error analysis and categorization
    âœ“ Data anonymization for compliance
    âœ“ Security audit and threat detection
    âœ“ Performance metrics extraction
    âœ“ Critical alert identification
    âœ“ Automated analysis pipeline

    **Skills Demonstrated:**

    **Regex Mastery:**
    - Email pattern matching
    - IP address extraction
    - Timestamp parsing
    - Error pattern recognition
    - Multi-pattern matching

    **grep Expertise:**
    - Extended regex (-E)
    - Pattern extraction (-o)
    - Context viewing (-A, -B, -C)
    - Inverted matching (-v)
    - Case-insensitive search (-i)

    **sed Proficiency:**
    - Global substitution
    - Pattern-based replacement
    - Data anonymization
    - Log transformation
    - Multi-step processing

    **Shell Scripting:**
    - Pipeline construction
    - Function organization
    - Error handling
    - Automation
    - Report generation

    **Professional Deliverables:**

    1. Error Summary Report
       - Statistical analysis
       - Component breakdown
       - Actionable insights

    2. Anonymized Logs
       - PII redaction
       - Structure preservation
       - Audit compliance

    3. Security Report
       - Threat identification
       - Risk assessment
       - Remediation steps

    4. Performance Metrics
       - Response time analysis
       - Status code distribution
       - Optimization recommendations

    5. Critical Alerts
       - Severity classification
       - Impact assessment
       - Immediate actions

    6. Automated Analysis Tool
       - Reusable script
       - Consistent reporting
       - Dashboard generation

    **Real-World Applications:**

    âœ“ Incident response
    âœ“ Security auditing
    âœ“ Performance monitoring
    âœ“ Compliance reporting
    âœ“ Capacity planning
    âœ“ Troubleshooting
    âœ“ SLA verification

    **Professional Workflow:**

    1. Log Collection
    2. Pattern Identification
    3. Data Extraction
    4. Analysis & Correlation
    5. Report Generation
    6. Action Recommendations
    7. Automation & Monitoring

    **Key Techniques Mastered:**

    Pattern Matching:
    - Email: [a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]+
    - IP: ([0-9]{1,3}\.){3}[0-9]{1,3}
    - Timestamp: [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}

    Data Anonymization:
    sed 's/email@domain/REDACTED/g'
    sed 's/[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/XXX.XXX.XXX.XXX/g'

    Statistical Analysis:
    grep pattern | wc -l
    sort | uniq -c | sort -rn
    awk '{sum+=$1} END {print sum/NR}'

    **Production-Ready Skills:**

    â˜‘ Regex pattern design
    â˜‘ Log parsing and transformation
    â˜‘ Security analysis
    â˜‘ Performance monitoring
    â˜‘ Report automation
    â˜‘ Data anonymization
    â˜‘ Incident detection

    **Best Practices Applied:**

    â˜‘ Test patterns before automation
    â˜‘ Preserve log structure during anonymization
    â˜‘ Provide actionable recommendations
    â˜‘ Calculate business impact
    â˜‘ Prioritize by severity
    â˜‘ Automate repetitive tasks
    â˜‘ Generate executive summaries

    **Next Level Skills:**

    - ELK Stack (Elasticsearch, Logstash, Kibana)
    - Splunk log analysis
    - CloudWatch Logs Insights
    - Real-time log streaming
    - Machine learning for anomaly detection
    - Distributed tracing
    - Log aggregation at scale

    You now have professional-grade log analysis skills used in
    production environments at major tech companies!

    This expertise is critical for:
    - DevOps Engineers
    - Site Reliability Engineers (SRE)
    - Security Analysts
    - System Administrators
    - Platform Engineers

    EXCEPTIONAL WORK! ğŸš€
  xp: 700
  unlocks:
    - "linux/week14/ssh-basics-beginner"
    - "linux/week14/ssh-advanced"
